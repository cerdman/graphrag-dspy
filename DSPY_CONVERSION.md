# GraphRAG DSPy Conversion

This document describes the conversion of GraphRAG's prompt and model execution mechanisms to use [DSPy](https://dspy.ai/), a framework for programming language models.

## Overview

GraphRAG has been enhanced to use DSPy for prompt execution while maintaining backward compatibility with the existing prompt-based approach. DSPy provides:

- **Declarative Signatures**: Type-safe input/output definitions for LM tasks
- **Composable Modules**: Reusable components with automatic prompt optimization
- **Better Organization**: Structured approach to prompt engineering
- **Claude Support**: Native integration with Claude and other LLMs

## Architecture

### DSPy Integration Components

1. **DSPy LM Adapter** (`graphrag/language_model/providers/dspy/adapter.py`)
   - Bridges GraphRAG's `ChatModel` protocol with DSPy's `LM` interface
   - Enables GraphRAG's existing LM implementations to work with DSPy modules
   - Handles async/sync conversion automatically

2. **DSPy Modules** (`graphrag/dspy_modules/`)
   - `index.py`: Modules for index operations (Graph Extraction, Community Reports, etc.)
   - `query.py`: Modules for query operations (Local Search, Global Search, etc.)
   - `claude_support.py`: Helper utilities for Claude model integration

3. **Updated Extractors**
   - Graph Extractor: Uses `GraphExtractionModule` (DSPy) or legacy prompts
   - Community Reports Extractor: Uses `CommunityReportModule` (DSPy) or legacy prompts
   - Both support `use_dspy=True/False` flag for gradual migration

## Usage

### Enabling DSPy (Default)

By default, DSPy is enabled for all extractors:

```python
from graphrag.index.operations.extract_graph.graph_extractor import GraphExtractor

extractor = GraphExtractor(model_invoker=chat_model)  # Uses DSPy by default
```

### Using Legacy Prompts

To use the original prompt-based approach:

```python
extractor = GraphExtractor(model_invoker=chat_model, use_dspy=False)
```

### Claude Support

To use Claude models with GraphRAG:

```python
import os
import dspy
from graphrag.dspy_modules.claude_support import configure_claude

# Set your API key
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"

# Configure Claude
lm = configure_claude(model="anthropic/claude-sonnet-4-20250514")

# Now all DSPy operations will use Claude
# GraphRAG extractors will automatically use Claude when use_dspy=True
```

### Available Claude Models

- `anthropic/claude-sonnet-4-20250514` (recommended)
- `anthropic/claude-3-7-sonnet-20250219`
- `anthropic/claude-3-5-haiku-20241022`
- `anthropic/claude-3-opus-20240229`

## DSPy Modules Reference

### Index Modules

#### GraphExtractionModule
- **Inputs**: entity_types, input_text, delimiters
- **Output**: extracted_data (entities and relationships)
- **Features**: Iterative refinement with gleanings

#### CommunityReportModule
- **Inputs**: input_text, max_report_length
- **Output**: Structured report (title, summary, findings, rating)
- **Features**: ChainOfThought reasoning for comprehensive reports

#### ClaimExtractionModule
- **Inputs**: input_text, entity_specs
- **Output**: extracted_claims
- **Features**: Focused claim extraction

#### DescriptionSummaryModule
- **Inputs**: descriptions (newline-separated)
- **Output**: summary
- **Features**: Concise summarization

### Query Modules

#### LocalSearchModule
- **Inputs**: context_data, response_type, question
- **Output**: answer with data references

#### GlobalSearchMapModule/GlobalSearchReduceModule
- **Map**: Analyze individual community reports
- **Reduce**: Synthesize analyses into final answer

#### DriftSearchModule
- **Inputs**: conversation_history, current_question, context_data
- **Output**: Context-aware answer handling conversation drift

#### QuestionGenModule
- **Inputs**: context, num_questions
- **Output**: Generated questions

## Migration Status

### Completed âœ…
- DSPy dependency added to `pyproject.toml`
- GraphRAGDSpyLM adapter created
- All DSPy signatures and modules implemented
- Graph Extraction converted to DSPy
- Community Reports converted to DSPy
- Claude support added

### In Progress ðŸš§
- Converting remaining extractors (Claims, Descriptions)
- Query operations integration
- Test suite validation

### Future Enhancements ðŸ”®
- DSPy prompt optimization (BootstrapFewShot, MIPRO)
- Fine-tuning signatures based on GraphRAG datasets
- Performance benchmarking (DSPy vs legacy)
- Additional LLM provider integrations

## Benefits of DSPy Conversion

1. **Type Safety**: Input/output schemas prevent runtime errors
2. **Composability**: Modules can be combined and reused
3. **Optimization**: DSPy can automatically improve prompts
4. **Maintainability**: Centralized prompt logic in signatures
5. **Flexibility**: Easy to swap LLM providers (OpenAI, Claude, etc.)
6. **Better Testing**: Mock DSPy modules for unit tests

## Backward Compatibility

All existing GraphRAG code continues to work without modification. The `use_dspy` flag allows gradual migration:

- Set `use_dspy=True` (default) for new DSPy-based execution
- Set `use_dspy=False` to use original prompt-based approach
- Both approaches produce equivalent results

## Contributing

When adding new prompt-based operations to GraphRAG:

1. Create a DSPy Signature defining inputs/outputs
2. Create a DSPy Module using the signature
3. Add to `graphrag/dspy_modules/index.py` or `query.py`
4. Update the corresponding extractor with `use_dspy` flag
5. Test both DSPy and legacy modes

## References

- DSPy Documentation: https://dspy.ai/learn/
- DSPy API Reference: https://dspy.ai/api/
- DSPy GitHub: https://github.com/stanfordnlp/dspy
- GraphRAG Documentation: https://microsoft.github.io/graphrag/

## Advanced DSPy Patterns Used

GraphRAG's DSPy modules demonstrate state-of-the-art LLM orchestration patterns that go far beyond simple prompt wrappers. Each module leverages DSPy's composition capabilities for more reliable and sophisticated reasoning.

### Multi-Agent Patterns

#### Graph Extraction (4-Agent Pipeline)
```python
GraphExtractionModule uses 4 specialized agents:
1. Entity Identifier (ChainOfThought) - Identifies entities with reasoning
2. Relationship Identifier (ChainOfThought) - Finds relationships
3. Refinement Agent (ReAct) - Iteratively improves extractions
4. Output Formatter (Predict) - Formats for GraphRAG

Flow: Sequential â†’ Iterative Refinement â†’ Format
```

**Why this works better:**
- Specialization: Each agent focuses on one task
- Reasoning: ChainOfThought provides explainable decisions
- Iteration: ReAct pattern catches missed entities
- Modularity: Each agent can be optimized independently

#### Community Reports (Parallel Analysis + Synthesis)
```python
CommunityReportModule uses 3 parallel analysts + synthesizer:
1. Structure Analyst - Analyzes entity connections
2. Impact Analyst - Assesses severity and impact  
3. Findings Analyst - Extracts key insights
4. Synthesizer - Combines all analyses

Flow: Parallel Analysis â†’ Synthesis
```

**Why this works better:**
- Diversity: Multiple perspectives on same data
- Efficiency: Parallel execution (can run concurrently)
- Comprehensiveness: Less likely to miss important aspects
- Quality: Synthesis resolves conflicts between views

### ReAct Patterns

#### Claim Extraction (Think-Act-Observe-Refine)
```python
ClaimExtractionModule:
1. Identifier (ChainOfThought) - Initial claim candidates
2. Validator (ChainOfThought) - Validates and refines
3. Loop until validation passes or max iterations

Flow: Identify â†’ Validate â†’ Refine â†’ Repeat
```

**Why this works better:**
- Self-correction: Catches initial mistakes
- Quality gates: Validation prevents bad outputs
- Adaptive: Stops when confident, continues when unsure

### Ensemble Patterns

#### Local Search (Multiple Reasoning Paths)
```python
LocalSearchModule uses ensemble reasoning:
1. Context Understander - Extracts relevant info
2. Direct Answerer - Quick factual response
3. Reasoned Answerer (ChainOfThought) - Deep reasoning
4. Synthesizer - Combines both answers

Flow: Understand â†’ Parallel Reasoning â†’ Synthesize
```

**Why this works better:**
- Robustness: If one path fails, others compensate
- Complementary: Direct + reasoned cover different cases
- Quality: Synthesis picks best from both worlds

#### Description Summary (Dual-Path)
```python
DescriptionSummaryModule:
1. Extractive Path - Preserves exact information
2. Abstractive Path - Synthesizes concepts
3. Fusion - Combines both approaches

Flow: Parallel Paths â†’ Fusion
```

**Why this works better:**
- Balance: Accuracy (extractive) + conciseness (abstractive)
- Completeness: Doesn't miss important details
- Readability: Fusion optimizes for human consumption

### Hierarchical Patterns

#### Global Search Map-Reduce

**Map with Filtering:**
```python
GlobalSearchMapModule:
1. Relevance Checker (ChainOfThought) - Scores relevance
2. Analyzer (ChainOfThought) - Analyzes if relevant
3. Skip if not relevant

Flow: Filter â†’ Analyze (conditional)
```

**Reduce with Self-Refinement:**
```python
GlobalSearchReduceModule:
1. Initial Synthesizer - Combines analyses
2. Refiner (Reflection) - Critiques and improves
3. Output refined answer

Flow: Synthesize â†’ Reflect â†’ Refine
```

**Why this works better:**
- Efficiency: Filtering saves compute on irrelevant data
- Quality: Self-refinement catches synthesis errors
- Scalability: Handles many communities gracefully

### Memory-Augmented Patterns

#### Drift Search (Conversational ReAct)
```python
DriftSearchModule:
1. Memory Extractor - Summarizes conversation
2. Drift Detector - Checks if topic changed
3. Contextual Answerer - Adapts to drift

Flow: Remember â†’ Detect â†’ Adapt
```

**Why this works better:**
- Context-aware: Maintains conversation thread
- Adaptive: Handles topic changes gracefully
- Personalized: Uses conversation history

### Divergence-Convergence Patterns

#### Question Generation
```python
QuestionGenModule:
1. Creative Generator - Over-generates diverse questions
2. Validator - Filters to answerable questions

Flow: Diverge (creative) â†’ Converge (validate)
```

**Why this works better:**
- Creativity: Over-generation explores possibilities
- Quality: Validation ensures answerability
- Diversity: Multiple perspectives covered

## Optimization Potential

All modules are ready for DSPy's optimization algorithms:

### BootstrapFewShot
Automatically generate training examples from your data:
```python
from dspy.teleprompt import BootstrapFewShot

# Optimize Graph Extraction
optimizer = BootstrapFewShot(metric=extraction_quality_metric)
optimized_extractor = optimizer.compile(
    GraphExtractionModule(),
    trainset=graphrag_examples
)
```

### MIPRO (Multi-prompt Instruction Proposal Optimizer)
Optimize both instructions and few-shot examples:
```python
from dspy.teleprompt import MIPRO

optimizer = MIPRO(
    metric=community_report_metric,
    num_candidates=10,
    init_temperature=1.0
)
optimized_reporter = optimizer.compile(
    CommunityReportModule(),
    trainset=community_examples,
    num_trials=100
)
```

### Ensemble Optimization
Combine multiple optimized variants:
```python
# Create ensemble of different optimization runs
ensemble = dspy.majority([
    optimized_extractor_1,
    optimized_extractor_2,
    optimized_extractor_3,
])
```

## Composition Examples

DSPy modules can be composed into larger programs:

```python
class GraphRAGPipeline(dspy.Module):
    """End-to-end GraphRAG pipeline."""
    
    def __init__(self):
        self.extractor = GraphExtractionModule()
        self.summarizer = DescriptionSummaryModule()
        self.reporter = CommunityReportModule()
    
    def forward(self, documents):
        # Extract entities and relationships
        graph = self.extractor(...)
        
        # Summarize descriptions
        summaries = self.summarizer(...)
        
        # Generate community reports
        reports = self.reporter(...)
        
        return dspy.Prediction(
            graph=graph,
            summaries=summaries,
            reports=reports
        )

# This entire pipeline can be optimized end-to-end!
optimizer = BootstrapFewShot(metric=pipeline_quality)
optimized_pipeline = optimizer.compile(
    GraphRAGPipeline(),
    trainset=full_examples
)
```

## Performance Considerations

### Parallel Execution
Many modules support parallel execution:
- Community Reports: 3 analysts run in parallel
- Local Search: Direct + reasoned paths in parallel
- Description Summary: Extractive + abstractive in parallel

Use DSPy's async capabilities:
```python
import asyncio

async def parallel_analysis():
    results = await asyncio.gather(
        analyst_1.forward(...),
        analyst_2.forward(...),
        analyst_3.forward(...),
    )
    return synthesize(results)
```

### Caching
DSPy automatically caches LM calls:
```python
# Same inputs = cached response (fast!)
result1 = module.forward(text="...")
result2 = module.forward(text="...")  # Cached, instant
```

### Tracing
Enable DSPy tracing to debug multi-agent flows:
```python
import dspy
dspy.configure(trace=True)

# Now you can see the full execution trace
result = GraphExtractionModule().forward(...)
# Prints: Agent 1 â†’ Agent 2 â†’ Agent 3 (iteration 1) â†’ ...
```

## Testing

Each DSPy module is independently testable:

```python
def test_graph_extraction():
    module = GraphExtractionModule(max_gleanings=1)
    
    result = module.forward(
        entity_types="PERSON,ORG",
        input_text="Alice works at TechCorp.",
        tuple_delimiter="<|>",
        record_delimiter="##",
        completion_delimiter="<|COMPLETE|>"
    )
    
    assert "ALICE" in result.extracted_data
    assert "TECHCORP" in result.extracted_data
```

Mock specific agents for unit tests:
```python
def test_refinement_agent():
    module = GraphExtractionModule()
    
    # Test just the refinement agent
    result = module.refinement_agent(
        entities="ALICE<|>PERSON<|>...",
        relationships="...",
        entity_types="PERSON,ORG"
    )
    
    assert hasattr(result, 'needs_refinement')
```

## Comparison: Before vs After

### Before (String Templates)
```python
PROMPT = """Extract entities from: {text}
Entity types: {types}"""

response = await model.achat(
    PROMPT.format(text=input_text, types="PERSON,ORG")
)
# Hope it works! ðŸ¤ž
```

### After (DSPy Multi-Agent)
```python
extractor = GraphExtractionModule()
result = extractor.forward(
    entity_types="PERSON,ORG",
    input_text=input_text
)
# 4 specialized agents, iterative refinement,
# type-safe, composable, optimizable! ðŸš€
```

The difference:
- âœ… Type safety
- âœ… Composability
- âœ… Testability
- âœ… Explainability (reasoning traces)
- âœ… Optimizability (auto-prompt tuning)
- âœ… Robustness (multi-agent, ensemble)

This is the power of programming LLMs, not just prompting them.

